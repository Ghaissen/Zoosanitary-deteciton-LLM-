{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghaissen/Zoosanitary-deteciton-LLM-/blob/main/web_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdt9nQMXXirs",
        "outputId": "42e834ec-c737-4a29-c958-4858b952c748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ngrams\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import urllib.request\n",
        "import requests\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from datetime import datetime\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, cross_validate\n",
        "from sklearn.decomposition import PCA,LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from textblob import TextBlob,Word\n",
        "from urllib.request import Request\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "wxffuK_EXlN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le dataset\n",
        "data_path = \"/content/drive/MyDrive/pcd/data_processed_PCD.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI6HTDFTXpmX",
        "outputId": "c4986cb9-668e-4844-ee66-2f6069e36563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Article language  probability  \\\n",
            "0  Crimean-Congo Haemorrhagic Fever Cases Increas...       en     0.999998   \n",
            "1  State confirms potentially deadly EEE virus in...       en     0.999995   \n",
            "2  Nagaland officially declared as Lumpy Skin Dis...       en     0.999997   \n",
            "3  Nagaland officially declared as Lumpy Skin Dis...       en     0.999998   \n",
            "4  Lumpy skin disease (LSD) is a notifiable disea...       en     1.000000   \n",
            "\n",
            "                                      processed_text  \n",
            "0  Crimean Congo Haemorrhagic Fever case Increase...  \n",
            "1  state confirm potentially deadly EEE virus Bar...  \n",
            "2  Nagaland officially declare Lumpy Skin Disease...  \n",
            "3  Nagaland officially declare Lumpy Skin Disease...  \n",
            "4  lumpy skin disease LSD notifiable disease acco...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 lxml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5hiUp7fbb6-",
        "outputId": "cdb9f57a-2076-454c-d61d-78fd13d0ce8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getlinks(soup, base_url):\n",
        "    l = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        # Check if the href is a full URL or a relative path\n",
        "        if not href.startswith('http'):\n",
        "            href = base_url + href  # Concatenate with base URL if it's a relative path\n",
        "        l.append(href)\n",
        "    return l\n",
        "\n",
        "def getdate(soup):\n",
        "    div = soup.find('label', class_='label')\n",
        "    return div.text if div else \"No Date Found\"\n",
        "#\n",
        "#def getname(soup):\n",
        " #   l = []\n",
        " #   div = soup.find('a', class_ = 'bbp-author-name')\n",
        " #   return([div.text])\n",
        "#\n",
        "def gettext(soup):\n",
        "    p = soup.find('section', class_='rte w-max')\n",
        "    return p.text if p else \"No Text Found\"\n",
        "#\n",
        "\n",
        "def get_yearly_links(base_url):\n",
        "    response = requests.get(base_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    yearly_links = [a['href'] for a in soup.find_all('a', class_='nav-link')]\n",
        "    return yearly_links\n",
        "\n",
        "\n",
        "sitelist = []\n",
        "site = 'https://www.ssi.dk/aktuelt/nyheder'\n",
        "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "yearly_links = get_yearly_links(site)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# This assumes each yearly link contains pages that need to be scraped\n",
        "for year_link in yearly_links:\n",
        "    print(f\"Scraping {year_link}\")\n",
        "    year_response = requests.get(year_link, headers=hdr)\n",
        "    year_soup = BeautifulSoup(year_response.text, 'html.parser')\n",
        "    links = getlinks(year_soup, site)\n",
        "    for link in links:\n",
        "      link_response = requests.get(link, headers=hdr)\n",
        "      if link_response.status_code == 200:\n",
        "        link_soup = BeautifulSoup(link_response.text, 'html.parser')\n",
        "        date = getdate(link_soup)\n",
        "        text = gettext(link_soup)\n",
        "        d = [{'DATE': date, 'TEXT': text, 'STATE': \"no event\"}]\n",
        "        df = df.append(pd.DataFrame(d), ignore_index=True)\n",
        "df.to_csv('/content/drive/MyDrive/pcd/articles.csv')"
      ],
      "metadata": {
        "id": "fPulz4CI17ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "id": "wHfvz2_XdPAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n"
      ],
      "metadata": {
        "id": "gj2iPIQ5cvLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le dataset\n",
        "data_path2 = \"/content/drive/MyDrive/pcd/articles.csv\"\n",
        "df2 = pd.read_csv(data_path2)\n",
        "print(df2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxO0mfrQdn_q",
        "outputId": "e5c97c9b-9fca-4529-96c4-53c6cd4e4dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5761, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2[df2['DATE'] != 'No Date Found']\n",
        "\n",
        "df2 = df2[df2['TEXT'] != 'No Text Found']\n",
        "\n",
        "df2.to_csv('/content/drive/MyDrive/pcd/articles_v2.csv', index=False)\n",
        "print(df2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpGpOBjyqIYH",
        "outputId": "f9d129de-d8bf-4bd6-f90f-45f07dd2890c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3805, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df2 = df2.sample(n=25, random_state=1)  # Changez 'random_state' pour différents échantillons\n",
        "df2.to_csv('/content/drive/MyDrive/pcd/articles_25.csv', index=False)\n",
        "print(df2.shape)"
      ],
      "metadata": {
        "id": "dMlXp0XrrCTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f2d468-aab9-4f52-e348-f34500ae29d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path3 = \"/content/drive/MyDrive/pcd/articles_25.csv\"\n",
        "df3 = pd.read_csv(data_path3)"
      ],
      "metadata": {
        "id": "Wt1Zmj1fumBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize translator\n",
        "translator = Translator()\n",
        "\n",
        "def safe_translate(text, dest_language):\n",
        "    try:\n",
        "        translation = translator.translate(text, dest=dest_language)\n",
        "        if translation and hasattr(translation, 'text'):\n",
        "            return translation.text\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    return text  # Return the original text if translation fails\n",
        "\n",
        "# Assuming 'df' is your DataFrame and it has a column named 'TEXT' with Danish texts\n",
        "df3['TEXT_EN'] = df3['TEXT'].apply(lambda text: safe_translate(text, 'en'))\n",
        "df3['TEXT_FR'] = df3['TEXT'].apply(lambda text: safe_translate(text, 'fr'))\n",
        "\n",
        "df3.to_csv('/content/drive/MyDrive/pcd/translated_articles.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9z1ne6bc6Nb",
        "outputId": "40eca0f8-d8a5-4034-974d-b1e536a16b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: the JSON object must be str, bytes or bytearray, not NoneType\n",
            "An error occurred: the JSON object must be str, bytes or bytearray, not NoneType\n",
            "An error occurred: the JSON object must be str, bytes or bytearray, not NoneType\n",
            "An error occurred: the JSON object must be str, bytes or bytearray, not NoneType\n",
            "An error occurred: the JSON object must be str, bytes or bytearray, not NoneType\n",
            "An error occurred: list index out of range\n",
            "An error occurred: the JSON object must be str, bytes or bytearray, not NoneType\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_disease_info(soup):\n",
        "    diseases_data = []\n",
        "    h3_tags = soup.find_all('h3')\n",
        "\n",
        "    for h3_tag in h3_tags:\n",
        "        disease_name = h3_tag.get_text(strip=True)\n",
        "        content_texts = []\n",
        "        text_content=''\n",
        "\n",
        "        # Iterate over the next siblings of the current h3 tag\n",
        "        for sibling in h3_tag.next_siblings:\n",
        "            if sibling.name == 'h3':\n",
        "                break  # Stop if the next h3 tag is found\n",
        "            if sibling.name == 'p':\n",
        "              text_content=text_content+' '\n",
        "              content_texts.append(sibling.get_text(strip=True))\n",
        "            elif sibling.name == 'ul':\n",
        "              text_content=text_content+' '\n",
        "              list_items = sibling.find_all('li')\n",
        "              list_contents = ' '.join(li.get_text(strip=True) for li in list_items)\n",
        "              content_texts.append(list_contents)\n",
        "\n",
        "        # Concatenate disease name with its content texts\n",
        "        full_text = ' '.join([disease_name] + content_texts)\n",
        "        diseases_data.append(full_text)\n",
        "\n",
        "    return diseases_data\n",
        "\n",
        "site = 'https://ar.wikipedia.org/wiki/%D9%85%D8%B1%D8%B6_%D8%A7%D9%84%D8%AF%D9%88%D8%A7%D8%AC%D9%86#%D8%A8%D8%B9%D8%B6_%D8%A7%D9%85%D8%B1%D8%A7%D8%B6_%D8%A7%D9%84%D8%AF%D9%88%D8%A7%D8%AC%D9%86'\n",
        "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "response = requests.get(site, headers=hdr)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Get disease info from the page\n",
        "disease_info_list = get_disease_info(soup)\n",
        "\n",
        "# Create DataFrame from the scraped data\n",
        "df4 = pd.DataFrame(disease_info_list, columns=['TEXT'])\n",
        "\n",
        "# Add a state column with a default value\n",
        "df4['STATE'] = \"no event\"\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_path = '/content/drive/MyDrive/pcd/articles_animal_disease.csv'  # Modify as needed for your path\n",
        "df4.to_csv(csv_path, index=False)\n"
      ],
      "metadata": {
        "id": "jvLJPH5W6o3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getlinks(soup, base_url):\n",
        "    l = []\n",
        "    table = soup.find('table', {'width': '60%'})\n",
        "    for link in table.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        # Prepend base_url if href is a relative path\n",
        "        if not href.startswith('http'):\n",
        "            href = base_url + href\n",
        "        l.append(href)\n",
        "    return l\n",
        "\n",
        "def gettext(soup):\n",
        "    content_div = soup.find('div', class_='mw-content-rtl mw-parser-output')\n",
        "    text_content = []\n",
        "    if content_div:\n",
        "        for elem in content_div.children:\n",
        "            if elem.name == 'meta' and elem.get('property', '') == 'mw:PageProp/toc':\n",
        "                break\n",
        "            if elem.name == 'p':\n",
        "              text_content.append(elem.get_text(strip=True))\n",
        "    return ' '.join(text_content) if text_content else \"No Text Found\"\n",
        "\n",
        "# Base URL definition for Wikipedia\n",
        "base_url = 'https://ar.wikipedia.org'\n",
        "site = base_url + '/wiki/%D8%B7%D8%A8_%D8%A8%D9%8A%D8%B7%D8%B1%D9%8A'\n",
        "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "\n",
        "# Request the main page to get links from\n",
        "response = requests.get(site, headers=hdr)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "links = getlinks(soup, base_url)\n",
        "\n",
        "# Data list to hold the dictionaries before creating DataFrame\n",
        "data_list = []\n",
        "\n",
        "# Loop through each link and scrape the text\n",
        "for link in links:\n",
        "    print(f\"Scraping {link}\")\n",
        "    link_response = requests.get(link, headers=hdr)\n",
        "    if link_response.status_code == 200:\n",
        "        link_soup = BeautifulSoup(link_response.text, 'html.parser')\n",
        "        text = gettext(link_soup)\n",
        "        data_list.append({'TEXT': text, 'STATE': \"no event\"})\n",
        "\n",
        "# Create DataFrame from the list of data\n",
        "df5 = pd.DataFrame(data_list)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "# Change the path according to your system's directory structure\n",
        "csv_path = '/content/drive/MyDrive/pcd/animal_diseases_2.csv'\n",
        "df5.to_csv(csv_path, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4eB4PYYTMeG",
        "outputId": "6cc32619-f37a-4291-8e20-4597ff7d78a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping https://ar.wikipedia.org/wiki/%D8%AD%D9%85%D9%89_%D8%A7%D9%84%D9%88%D8%A7%D8%AF%D9%8A_%D8%A7%D9%84%D9%85%D8%AA%D8%B5%D8%AF%D8%B9\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%A5%D9%86%D9%81%D9%84%D9%88%D9%86%D8%B2%D8%A7_%D8%A7%D9%84%D8%B7%D9%8A%D9%88%D8%B1\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AD%D9%85%D9%89_%D9%82%D9%84%D8%A7%D8%B9%D9%8A%D8%A9\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AF%D8%A7%D8%A1_%D8%A7%D9%84%D9%83%D9%84%D8%A8\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AC%D8%B1%D8%A8_%D8%A7%D9%84%D8%A5%D8%A8%D9%84\n",
            "Scraping https://ar.wikipedia.org/wiki/%D9%86%D8%BA%D9%81\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AF%D8%A7%D8%A1_%D8%A7%D9%84%D9%85%D8%AA%D9%88%D8%B1%D9%82%D8%A7%D8%AA\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AF%D8%A7%D8%A1_%D8%A7%D9%84%D8%BA%D9%88%D9%85%D8%A8%D9%88%D8%B1%D9%88\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%A5%D9%86%D9%81%D9%84%D9%88%D9%86%D8%B2%D8%A7_%D8%A7%D9%84%D8%AE%D9%8A%D9%88%D9%84\n",
            "Scraping https://ar.wikipedia.org/wiki/%D9%85%D8%B1%D8%B6_%D8%A7%D9%84%D9%84%D8%B3%D8%A7%D9%86_%D8%A7%D9%84%D8%A3%D8%B2%D8%B1%D9%82\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AC%D9%86%D9%88%D9%86_%D8%A7%D9%84%D8%A8%D9%82%D8%B1\n",
            "Scraping https://ar.wikipedia.org/wiki/%D9%85%D8%B1%D8%B6_%D8%AD%D9%8A%D9%88%D8%A7%D9%86%D9%8A_%D8%A7%D9%84%D9%85%D9%86%D8%B4%D8%A3\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AF%D8%A7%D8%A1_%D8%A7%D9%84%D9%85%D8%B4%D9%88%D9%83%D8%A7%D8%AA\n",
            "Scraping https://ar.wikipedia.org/w/index.php?title=%D9%85%D8%B1%D8%B6_%D8%A7%D9%84%D8%A7%D9%84%D8%AA%D9%87%D8%A7%D8%A8_%D8%A7%D9%84%D8%B1%D8%A6%D9%88%D9%8A_%D9%81%D9%8A_%D8%A7%D9%84%D8%A5%D8%A8%D9%84&action=edit&redlink=1\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AC%D8%AF%D8%B1%D9%8A\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AC%D8%B1%D8%A8\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%A5%D8%B3%D9%87%D8%A7%D9%84_%D9%81%D9%8A%D8%B1%D9%88%D8%B3%D9%8A_%D8%B9%D9%86%D8%AF_%D8%A7%D9%84%D8%A3%D8%A8%D9%82%D8%A7%D8%B1\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%B3%D9%88%D8%A7%D9%81\n",
            "Scraping https://ar.wikipedia.org/wiki/%D9%83%D9%88%D8%B1%D9%8A%D8%B2%D8%A7\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%B7%D8%A7%D8%B9%D9%88%D9%86_%D8%A7%D9%84%D9%85%D8%AC%D8%AA%D8%B1%D8%A7%D8%AA_%D8%A7%D9%84%D8%B5%D8%BA%D9%8A%D8%B1%D8%A9\n",
            "Scraping https://ar.wikipedia.org/wiki/%D9%85%D8%AB%D9%82%D8%A8%D9%8A%D8%A9\n",
            "Scraping https://ar.wikipedia.org/wiki/%D9%85%D8%AA%D9%84%D8%A7%D8%B2%D9%85%D8%A9_%D8%A7%D9%84%D8%A3%D9%86%D9%81_%D8%A7%D9%84%D8%A3%D8%A8%D9%8A%D8%B6\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AF%D8%A7%D8%A1_%D8%A7%D9%84%D8%A8%D8%B1%D9%88%D8%B3%D9%8A%D9%84%D8%A7%D8%AA\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AC%D9%85%D8%B1%D8%A9_%D8%AE%D8%A8%D9%8A%D8%AB%D8%A9\n",
            "Scraping https://ar.wikipedia.org/wiki/%D9%85%D8%B1%D8%B6_%D8%AE%D8%AF%D8%B4_%D8%A7%D9%84%D9%82%D8%B7%D8%A9\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%B7%D8%A7%D8%B9%D9%88%D9%86\n",
            "Scraping https://ar.wikipedia.org/wiki/%D9%85%D9%84%D8%A7%D8%B1%D9%8A%D8%A7\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AD%D9%85%D9%89_%D8%A7%D9%84%D8%AA%D9%8A%D9%81%D9%88%D8%A6%D9%8A%D8%AF\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AF%D8%A7%D8%A1_%D8%A7%D9%84%D9%85%D9%82%D9%88%D8%B3%D8%A7%D8%AA\n",
            "Scraping https://ar.wikipedia.org/wiki/%D8%AF%D8%A7%D8%A1_%D9%86%D9%8A%D9%88%D9%83%D8%A7%D8%B3%D9%84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNxWg9UQiJBU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}